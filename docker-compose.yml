volumes:
  ollama_storage:
    driver: local
  huggingface_cache:
    driver: local

services:
  ollama-app:
    image: ollama/ollama:latest
    container_name: ollama-app
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 10s
      timeout: 5s
      retries: 10
    #environment:
      # experimental features to decrease RAM usage:
      #- OLLAMA_FLASH_ATTENTION=true
      #- OLLAMA_KV_CACHE_TYPE=f16
      #- OLLAMA_MAX_LOADED_MODELS=1

  app:
    profiles: ["cpu"]
    build: .
    container_name: streamlit-app
    restart: unless-stopped
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - OLLAMA_BASE_URL=http://ollama-app:11434
    depends_on:
      - ollama-app

  # ðŸŽ® GPU-ENABLED VERSION (RTX 4090, RTX 3060, etc.)
  # =================================================
  # Same as above but with GPU support enabled
  # Use: docker-compose --profile gpu up --build
  app-gpu:
    profiles: ["gpu"]
    build: .
    container_name: streamlit-app-gpu
    restart: unless-stopped
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - huggingface_cache:/root/.cache/huggingface
    environment:
      - OLLAMA_BASE_URL=http://ollama-app:11434
    depends_on:
      - ollama-app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  ollama-pull-models:
    image: ollama/ollama:latest
    container_name: ollama-pull-models
    volumes:
      - ollama_storage:/root/.ollama
    entrypoint: /bin/sh
    command:
      - "-c"
      - |
        # Wait for Ollama service to be ready
        sleep 5
        
        # Pull large language models
        #OLLAMA_HOST=ollama-app:11434 ollama pull qwen3:30b-a3b-q4_K_M
        #OLLAMA_HOST=ollama-app:11434 ollama pull gemma3:12b-it-qat
        OLLAMA_HOST=ollama-app:11434 ollama pull gemma:2b
        OLLAMA_HOST=ollama-app:11434 ollama pull llama3.2:1b