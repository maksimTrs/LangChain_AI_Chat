volumes:
  ollama_storage:
    driver: local

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - 11434:11434
    volumes:
      - ollama_storage:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 10s
      timeout: 5s
      retries: 10
    #environment:
      # experimental features to decrease RAM usage:
      #- OLLAMA_FLASH_ATTENTION=true
      #- OLLAMA_KV_CACHE_TYPE=f16
      #- OLLAMA_MAX_LOADED_MODELS=1

  ollama-pull-models:
    image: ollama/ollama:latest
    container_name: ollama-pull-llama
    volumes:
      - ollama_storage:/root/.ollama
    entrypoint: /bin/sh
    command:
      - "-c"
      - |
        # Wait for Ollama service to be ready
        sleep 3
        
        # Pull large language models
        OLLAMA_HOST=ollama:11434 ollama pull qwen3:30b-a3b-q4_K_M
        OLLAMA_HOST=ollama:11434 ollama pull gemma3:12b-it-qat
        
        # Pull embedding model
        OLLAMA_HOST=ollama:11434 ollama pull nomic-embed-text
    depends_on:
      - ollama